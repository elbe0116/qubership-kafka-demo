name: Install Kafka

on:
  workflow_dispatch:
    inputs:
      environment:
        description: "Environment Name"
        required: false
        default: "dev"
      test_tags:
        description: "Test tags to run (e.g. kafka_crud, kafka_backup, or empty for all)"
        required: false
        default: ""

jobs:
  Install-Kafka:
    environment: "${{github.event.inputs.environment}}"
    runs-on: ubuntu-latest

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Listing downloaded files
        run: |
          pwd
          ls -R ./

      - name: Setup kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.32.0'

      - name: Setup Helm
        uses: azure/setup-helm@v3

      - name: Verify tools
        run: |
          kubectl version --client
          helm version
          aws --version

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ vars.AWS_REGION || 'us-east-1' }}

      - name: Configure kubeconfig for EKS
        run: |
          aws eks update-kubeconfig --region ${{ vars.AWS_REGION || 'us-east-1' }} --name ${{ vars.AWS_CLUSTERNAME }}

      - name: Verify cluster access
        run: kubectl get nodes

      - name: Install Kafka Cluster
        run: |
          echo "Clean previously installed Kafka"
          kubectl delete ns ${{ vars.KAFKA_INSTALL_NAMESPACE }} || true
          echo "Namespace to install: ${{ vars.KAFKA_INSTALL_NAMESPACE }}"
          
          # Install Kafka broker first
          cat > /tmp/kafka-values.yaml << 'EOF'
          global:
            tls:
              enabled: false
            securityContext:
              fsGroup: 1000
            secrets:
              kafka:
                adminUsername: "admin"
                adminPassword: "admin123"
                clientUsername: "client"
                clientPassword: "client123"
          operator:
            dockerImage: ghcr.io/netcracker/qubership-kafka-operator:main
          kafka:
            install: true
            dockerImage: ghcr.io/netcracker/qubership-docker-kafka:main
            kraft:
              enabled: true
            tls:
              enabled: false
            storage:
              size: 5Gi
              className:
                - gp2
            resources:
              requests:
                cpu: 100m
                memory: 512Mi
              limits:
                cpu: 500m
                memory: 1Gi
            securityContext:
              fsGroup: 1000
              runAsUser: 1000
          EOF
          
          helm install --namespace=${{ vars.KAFKA_INSTALL_NAMESPACE }} \
            --create-namespace \
            -f ./operator/charts/helm/kafka/values.yaml \
            -f /tmp/kafka-values.yaml \
            kafka ./operator/charts/helm/kafka
          
          echo "Waiting for Kafka cluster to be ready..."
          kubectl wait --for=condition=ready pod -l component=kafka -n ${{ vars.KAFKA_INSTALL_NAMESPACE }} --timeout=600s || true
          
      - name: Install Kafka Services
        run: |
          # Build helm arguments dynamically (only add non-empty values)
          HELM_ARGS=""
          
          # Enable integration tests and set test tags (empty = all tests)
          HELM_ARGS="$HELM_ARGS --set integrationTests.install=true"
          HELM_ARGS="$HELM_ARGS --set integrationTests.tags=${{ github.event.inputs.test_tags }}"
          # Use custom image with S3 upload scripts
          HELM_ARGS="$HELM_ARGS --set integrationTests.image=ghcr.io/elbe0116/qubership-kafka-integration-tests:main"
          
          # Secrets - S3 credentials for test results upload
          [ -n "${{ secrets.S3_AWS_ACCESS_KEY_ID }}" ] && HELM_ARGS="$HELM_ARGS --set integrationTests.atpStorage.username=${{ secrets.S3_AWS_ACCESS_KEY_ID }}"
          [ -n "${{ secrets.S3_AWS_SECRET_ACCESS_KEY }}" ] && HELM_ARGS="$HELM_ARGS --set integrationTests.atpStorage.password=${{ secrets.S3_AWS_SECRET_ACCESS_KEY }}"
          
          # Secrets - S3 credentials for backup daemon
          [ -n "${{ secrets.S3_AWS_ACCESS_KEY_ID }}" ] && HELM_ARGS="$HELM_ARGS --set backupDaemon.s3Config.keyId=${{ secrets.S3_AWS_ACCESS_KEY_ID }}"
          [ -n "${{ secrets.S3_AWS_SECRET_ACCESS_KEY }}" ] && HELM_ARGS="$HELM_ARGS --set backupDaemon.s3Config.keySecret=${{ secrets.S3_AWS_SECRET_ACCESS_KEY }}"
          
          # Variables (only add if set, otherwise use values.yaml defaults)
          [ -n "${{ vars.ATP_STORAGE_PROVIDER }}" ] && HELM_ARGS="$HELM_ARGS --set integrationTests.atpStorage.provider=${{ vars.ATP_STORAGE_PROVIDER }}"
          [ -n "${{ vars.ATP_STORAGE_SERVER_URL }}" ] && HELM_ARGS="$HELM_ARGS --set integrationTests.atpStorage.serverUrl=${{ vars.ATP_STORAGE_SERVER_URL }}"
          [ -n "${{ vars.ATP_STORAGE_SERVER_UI_URL }}" ] && HELM_ARGS="$HELM_ARGS --set integrationTests.atpStorage.serverUiUrl=${{ vars.ATP_STORAGE_SERVER_UI_URL }}"
          [ -n "${{ vars.ATP_STORAGE_BUCKET }}" ] && HELM_ARGS="$HELM_ARGS --set integrationTests.atpStorage.bucket=${{ vars.ATP_STORAGE_BUCKET }}"
          [ -n "${{ vars.AWS_REGION }}" ] && HELM_ARGS="$HELM_ARGS --set integrationTests.atpStorage.region=${{ vars.AWS_REGION }}"
          [ -n "${{ vars.ATP_REPORT_VIEW_UI_URL }}" ] && HELM_ARGS="$HELM_ARGS --set integrationTests.atpReportViewUiUrl=${{ vars.ATP_REPORT_VIEW_UI_URL }}"
          
          # Environment name (use input or variable)
          ENV_NAME="${{ vars.ENVIRONMENT_NAME }}"
          [ -z "$ENV_NAME" ] && ENV_NAME="${{ github.event.inputs.environment }}"
          [ -n "$ENV_NAME" ] && HELM_ARGS="$HELM_ARGS --set integrationTests.environmentName=$ENV_NAME"
          
          # MONITORED_IMAGES for kafka_images test
          # Using temp file to avoid shell escaping issues with spaces and special chars
          cat > /tmp/monitored-images-values.yaml << 'EOF'
          integrationTests:
            monitoredImages: "deployment kafka-service-operator kafka-service-operator ghcr.io:443/netcracker/qubership-kafka-service-operator:main,deployment kafka-1 kafka ghcr.io:443/netcracker/qubership-docker-kafka:main,deployment kafka-2 kafka ghcr.io:443/netcracker/qubership-docker-kafka:main,deployment kafka-3 kafka ghcr.io:443/netcracker/qubership-docker-kafka:main,deployment kafka-monitoring kafka-monitoring ghcr.io:443/netcracker/qubership-kafka-monitoring:main,deployment kafka-backup-daemon kafka-backup-daemon ghcr.io:443/netcracker/qubership-kafka-backup-daemon:main,deployment kafka-integration-tests-runner kafka-integration-tests-runner ghcr.io:443/elbe0116/qubership-kafka-integration-tests:main"
          EOF
          HELM_ARGS="$HELM_ARGS -f /tmp/monitored-images-values.yaml"
          
          # AWS EKS configuration
          # Use temp file to avoid shell escaping issues
          cat > /tmp/aws-values.yaml << 'EOF'
          global:
            tls:
              enabled: false
            securityContext:
              fsGroup: 1000
            secrets:
              kafka:
                adminUsername: "admin"
                adminPassword: "admin123"
                clientUsername: "client"
                clientPassword: "client123"
          operator:
            dockerImage: ghcr.io/netcracker/qubership-kafka-service-operator:main
          kafka:
            dockerImage: ghcr.io/netcracker/qubership-docker-kafka:main
            tls:
              enabled: false
            storage:
              className:
                - gp2
            resources:
              requests:
                cpu: 100m
                memory: 512Mi
              limits:
                cpu: 500m
                memory: 1Gi
            securityContext:
              fsGroup: 1000
              runAsUser: 1000
          monitoring:
            dockerImage: ghcr.io/netcracker/qubership-kafka-monitoring:main
          backupDaemon:
            dockerImage: ghcr.io/netcracker/qubership-kafka-backup-daemon:main
            install: true
            tls:
              enabled: false
            s3Config:
              enabled: true
              url: "https://s3.us-east-1.amazonaws.com"
              bucket: "qstp-kafka"
              region: "us-east-1"
            securityContext:
              fsGroup: 1000
          integrationTests:
            affinity: null
          EOF
          HELM_ARGS="$HELM_ARGS -f /tmp/aws-values.yaml"
          
          echo "Helm additional args: $HELM_ARGS"
          
          # Debug: show key values from values.yaml
          echo "=== Debug: Key values ==="
          grep -E "^\s*(install|enabled):" ./operator/charts/helm/kafka-service/values.yaml | head -20
          echo "========================="
          
          helm install --namespace=${{ vars.KAFKA_INSTALL_NAMESPACE }} \
            -f ./operator/charts/helm/kafka-service/values.yaml \
            kafka-service ./operator/charts/helm/kafka-service \
            $HELM_ARGS
